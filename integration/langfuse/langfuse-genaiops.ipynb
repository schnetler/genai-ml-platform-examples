{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "See the [AWS Partner Network (APN) Blog](https://aws.amazon.com/blogs/apn/transform-large-language-model-observability-with-langfuse/) for more information about the AWS partnership with Langfuse.\n",
    "\n",
    "> If you haven't selected the kernel, please click on the \"Select Kernel\" button at the upper right corner and select a Python Environment\".\n",
    ">\n",
    "> To execute each notebook cell, press `Shift + Enter`.\n",
    "\n",
    "## Option 1: Self-hosting\n",
    "Follow [this guide](https://github.com/aws-samples/deploy-langfuse-on-ecs-with-fargate/tree/main) to deploy Langfuse on Amazon ECS and Aurora.\n",
    "\n",
    "## Option 2: Managed Hosting\n",
    "Contact marketplace-aws@langfuse.com and subscribe to Langfuse plans through [AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=seller-nmyz7ju7oafxu).\n",
    "\n",
    "## Option 3: Langfuse cloud\n",
    "Sign up on [Langfuse Cloud](https://cloud.langfuse.com/) directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any of the above options, we will need to follow the [Langfuse Quickstart guide](https://langfuse.com/docs/get-started) to:\n",
    "1. Create a new project\n",
    "2. Create new API credentials in the project settings\n",
    "\n",
    "Once we obtain the API keys, we can:\n",
    "- Define them as environment variables inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment variables\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-...\"  # Your Langfuse project secret key\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-...\"  # Your Langfuse project public key\n",
    "os.environ[\"LANGFUSE_HOST\"] = (\n",
    "    \"https://xx.cloud.langfuse.com\"  # Region-specific Langfuse domain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or define the following in the `.env` file:\n",
    "    ```text\n",
    "    LANGFUSE_SECRET_KEY=sk-lf-... # Your Langfuse project secret key\n",
    "    \n",
    "    LANGFUSE_PUBLIC_KEY=pk-lf-... # Your Langfuse project public key\n",
    "    \n",
    "    LANGFUSE_HOST=https://xxx.xxx.awsapprunner.com # App Runner domain\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Dependencies\n",
    "\n",
    "We will use the `langfuse`, `boto3` and `litellm` Python packages. Specifically, we will use:\n",
    "\n",
    "- The `langfuse` SDK along with the public or self-hosting deployment to debug and improve LLM applications by tracing model invocations, managing prompts / models configurations and running evaluations.\n",
    "- The `boto3` SDK to interact with models on Amazon Bedrock or Amazon SageMaker.\n",
    "- (Optional) The `litellm` SDK to route requests to different LLM models with advanced load balancing and fallback, as well as standardizing the responses for chat, streaming, function calling and more.\n",
    "\n",
    "Note that you can also use other frameworks like LangChain or implement your own proxy instead of using `litellm`.\n",
    "\n",
    "Run the following command to install the required Python SDKs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langfuse==2.60.7 boto3==1.38.25 litellm==1.71.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Authentication Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a region for the Nova models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = \"us-west-2\"  # or \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# used to access Bedrock configuration\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=region_name)\n",
    "\n",
    "# Check if Nova models are available in this region\n",
    "models = bedrock.list_inference_profiles()\n",
    "nova_found = False\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "    if (\n",
    "        \"Nova Pro\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Lite\" in model[\"inferenceProfileName\"]\n",
    "        or \"Nova Micro\" in model[\"inferenceProfileName\"]\n",
    "    ):\n",
    "        print(\n",
    "            f\"Found Nova model: {model['inferenceProfileName']} - {model['inferenceProfileId']}\"\n",
    "        )\n",
    "        nova_found = True\n",
    "if not nova_found:\n",
    "    raise ValueError(\n",
    "        \"Nova models not found in available models. Please ensure you have access to Nova models.\",\n",
    "        \"Check you have enabled the models in the Bedrock console or the IAM role you are assuming.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client and check credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse has been set up correctly\")\n",
    "    print(f\"You can access your Langfuse instance at: {os.environ['LANGFUSE_HOST']}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Credentials not found or invalid. Check your Langfuse API key and host in the .env file or environment variable.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Gateway Options\n",
    "Choose one of the following options to invoke the Bedrock foundation models:\n",
    "* Option 1: Direct boto3, less abstraction, more control, more verbose\n",
    "* Option 2: LiteLLM proxy, more convenient, more built-in functionality, more abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Bedrock Converse API\n",
    "This option uses boto3 directly and does not require litellm. Best when only using Bedrock models. \n",
    "\n",
    "Refer to the [Carry out a conversation with the Converse API operations](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "from langfuse.model import PromptClient\n",
    "\n",
    "# used to invoke the Bedrock Converse API\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=region_name)\n",
    "\n",
    "# In case the input message is not in the Bedrock Converse API format,\n",
    "# for example it follow openAI format, we need to convert it to the Bedrock Converse API format.\n",
    "def convert_to_bedrock_messages(\n",
    "    messages: List[Dict[str, Any]],\n",
    ") -> Tuple[List[Dict[str, str]], List[Dict[str, Any]]]:\n",
    "    \"\"\"Convert message to Bedrock Converse API format\"\"\"\n",
    "    bedrock_messages = []\n",
    "\n",
    "    # Extract system messages first\n",
    "    system_prompts = []\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            system_prompts.append({\"text\": msg[\"content\"]})\n",
    "        else:\n",
    "            # Handle user/assistant messages\n",
    "            content_list = []\n",
    "\n",
    "            # If content is already a list, process each item\n",
    "            if isinstance(msg[\"content\"], list):\n",
    "                for content_item in msg[\"content\"]:\n",
    "                    if content_item[\"type\"] == \"text\":\n",
    "                        content_list.append({\"text\": content_item[\"text\"]})\n",
    "                    elif content_item[\"type\"] == \"image_url\":\n",
    "                        # Get image format from URL\n",
    "                        if \"url\" not in content_item[\"image_url\"]:\n",
    "                            raise ValueError(\n",
    "                                \"Missing required 'url' field in image_url\"\n",
    "                            )\n",
    "                        url = content_item[\"image_url\"][\"url\"]\n",
    "                        if not url:\n",
    "                            raise ValueError(\"URL cannot be empty\")\n",
    "                        parsed_url = urlparse(url)\n",
    "                        if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                            raise ValueError(\"Invalid URL format\")\n",
    "                        image_format = parsed_url.path.split(\".\")[-1].lower()\n",
    "                        # Convert jpg to jpeg for Bedrock compatibility\n",
    "                        if image_format == \"jpg\":\n",
    "                            image_format = \"jpeg\"\n",
    "\n",
    "                        # Download and encode image\n",
    "                        response = requests.get(url)\n",
    "                        image_bytes = response.content\n",
    "\n",
    "                        content_list.append(\n",
    "                            {\n",
    "                                \"image\": {\n",
    "                                    \"format\": image_format,\n",
    "                                    \"source\": {\"bytes\": image_bytes},\n",
    "                                }\n",
    "                            }\n",
    "                        )\n",
    "            else:\n",
    "                # If content is just text\n",
    "                content_list.append({\"text\": msg[\"content\"]})\n",
    "\n",
    "            bedrock_messages.append({\"role\": msg[\"role\"], \"content\": content_list})\n",
    "\n",
    "    return system_prompts, bedrock_messages\n",
    "\n",
    "\n",
    "@observe(as_type=\"generation\", name=\"Boto3 Wrapper\")\n",
    "def fn(\n",
    "    messages: List[Dict[str, Any]],\n",
    "    model_id: str = \"us.amazon.nova-pro-v1:0\",\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    metadata: Dict[str, Any] = {},\n",
    "    **kwargs,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Simple wrapper that only returns the response text.\n",
    "    \"\"\"\n",
    "    # 1. extract model metadata\n",
    "    kwargs_clone = kwargs.copy()\n",
    "    model_parameters = {\n",
    "        **kwargs_clone.pop(\"inferenceConfig\", {}),\n",
    "        **kwargs_clone.pop(\"additionalModelRequestFields\", {}),\n",
    "        **kwargs_clone.pop(\"guardrailConfig\", {}),\n",
    "    }\n",
    "    langfuse_context.update_current_observation(\n",
    "        input=messages,\n",
    "        model=model_id,\n",
    "        model_parameters=model_parameters,\n",
    "        prompt=prompt,\n",
    "    )\n",
    "\n",
    "    # Convert messages to Bedrock format\n",
    "    system_prompts, messages = convert_to_bedrock_messages(messages)\n",
    "\n",
    "    # 2. model call with error handling\n",
    "    try:\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            system=system_prompts,\n",
    "            messages=messages,\n",
    "            **kwargs,\n",
    "        )\n",
    "    except (ClientError, Exception) as e:\n",
    "        error_message = f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\"\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\", status_message=error_message\n",
    "        )\n",
    "        print(error_message)\n",
    "        return\n",
    "\n",
    "    # 3. extract response metadata\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    langfuse_context.update_current_observation(\n",
    "        output=response_text,\n",
    "        usage={\n",
    "            \"input\": response[\"usage\"][\"inputTokens\"],\n",
    "            \"output\": response[\"usage\"][\"outputTokens\"],\n",
    "            \"total\": response[\"usage\"][\"totalTokens\"],\n",
    "        },\n",
    "        metadata={\n",
    "            \"ResponseMetadata\": response[\"ResponseMetadata\"],\n",
    "            **metadata,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response_text\n",
    "\n",
    "@observe(as_type=\"generation\", name=\"Boto3 Streaming Wrapper\")\n",
    "def streaming_fn(\n",
    "    messages: List[Dict[str, Any]],\n",
    "    model_id: str = \"us.amazon.nova-pro-v1:0\",\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    metadata: Dict[str, Any] = {},\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple streaming wrapper that only yields text chunks.\n",
    "    \"\"\"\n",
    "    # 1. extract model metadata\n",
    "    kwargs_clone = kwargs.copy()\n",
    "    model_parameters = {\n",
    "        **kwargs_clone.pop(\"inferenceConfig\", {}),\n",
    "        **kwargs_clone.pop(\"additionalModelRequestFields\", {}),\n",
    "        **kwargs_clone.pop(\"guardrailConfig\", {}),\n",
    "    }\n",
    "    langfuse_context.update_current_observation(\n",
    "        input=messages,\n",
    "        model=model_id,\n",
    "        model_parameters=model_parameters,\n",
    "        prompt=prompt,\n",
    "    )\n",
    "\n",
    "    # Convert messages to Bedrock format\n",
    "    system_prompts, bedrock_messages = convert_to_bedrock_messages(messages)\n",
    "\n",
    "    # 2. Initialize variables to collect streaming response\n",
    "    full_response = \"\"\n",
    "    usage_data = {}\n",
    "    \n",
    "    try:\n",
    "        # 3. Call the streaming API\n",
    "        response = bedrock_runtime.converse_stream(\n",
    "            modelId=model_id,\n",
    "            system=system_prompts,\n",
    "            messages=bedrock_messages,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        stream = response.get('stream')\n",
    "        if stream:\n",
    "            for event in stream:\n",
    "                # Only yield text content\n",
    "                if 'contentBlockDelta' in event:\n",
    "                    chunk_text = event['contentBlockDelta']['delta']['text']\n",
    "                    full_response += chunk_text\n",
    "                    yield chunk_text\n",
    "                \n",
    "                # Collect usage information for Langfuse\n",
    "                elif 'metadata' in event and 'usage' in event['metadata']:\n",
    "                    event_metadata = event['metadata']\n",
    "                    usage_data = {\n",
    "                        \"input\": event_metadata['usage']['inputTokens'],\n",
    "                        \"output\": event_metadata['usage']['outputTokens'],\n",
    "                        \"total\": event_metadata['usage']['totalTokens'],\n",
    "                    }\n",
    "\n",
    "        # 4. Update Langfuse with final response data\n",
    "        langfuse_context.update_current_observation(\n",
    "            output=full_response,\n",
    "            usage=usage_data,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        error_message = f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\"\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\", status_message=error_message\n",
    "        )\n",
    "        print(error_message)\n",
    "        return\n",
    "\n",
    "@observe(as_type=\"generation\", name=\"Boto3 Tool Use Wrapper\")\n",
    "def tool_use_fn(\n",
    "    messages: List[Dict[str, str]],\n",
    "    tools: List[Dict[str, str]],\n",
    "    tool_choice: str = \"auto\",\n",
    "    model_id: str = \"us.amazon.nova-pro-v1:0\",\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    metadata: Dict[str, Any] = {},\n",
    "    **kwargs,\n",
    ") -> Optional[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Simple wrapper that only returns the tool calls.\n",
    "    \"\"\"\n",
    "    # 1. extract model metadata\n",
    "    kwargs_clone = kwargs.copy()\n",
    "    model_parameters = {\n",
    "        **kwargs_clone.pop(\"inferenceConfig\", {}),\n",
    "        **kwargs_clone.pop(\"additionalModelRequestFields\", {}),\n",
    "        **kwargs_clone.pop(\"guardrailConfig\", {}),\n",
    "    }\n",
    "\n",
    "    langfuse_context.update_current_observation(\n",
    "        input={\"messages\": messages, \"tools\": tools, \"tool_choice\": tool_choice},\n",
    "        model=model_id,\n",
    "        model_parameters=model_parameters,\n",
    "        prompt=prompt,\n",
    "    )\n",
    "\n",
    "    # Convert messages to Bedrock format\n",
    "    system_prompts, messages = convert_to_bedrock_messages(messages)\n",
    "\n",
    "    # 2. Convert tools to Bedrock format\n",
    "    tool_config = {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"toolSpec\": {\n",
    "                    \"name\": tool[\"function\"][\"name\"],\n",
    "                    \"description\": tool[\"function\"][\"description\"],\n",
    "                    \"inputSchema\": {\"json\": tool[\"function\"][\"parameters\"]},\n",
    "                }\n",
    "            }\n",
    "            for tool in tools\n",
    "            if tool[\"type\"] == \"function\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Add toolChoice configuration based on input\n",
    "    if tool_choice != \"auto\":\n",
    "        tool_config[\"toolChoice\"] = {\n",
    "            \"any\": {} if tool_choice == \"any\" else None,\n",
    "            \"auto\": {} if tool_choice == \"auto\" else None,\n",
    "            \"tool\": (\n",
    "                {\"name\": tool_choice} if not tool_choice in [\"any\", \"auto\"] else None\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    # 3. model call with error handling\n",
    "    try:\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            system=system_prompts,\n",
    "            messages=messages,\n",
    "            toolConfig=tool_config,\n",
    "            **kwargs,\n",
    "        )\n",
    "    except (ClientError, Exception) as e:\n",
    "        error_message = f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\"\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\", status_message=error_message\n",
    "        )\n",
    "        print(error_message)\n",
    "        return\n",
    "\n",
    "    # 4. Handle tool use flow if needed\n",
    "    output_message = response[\"output\"][\"message\"]\n",
    "\n",
    "    tool_calls = []\n",
    "    if response[\"stopReason\"] == \"tool_use\":\n",
    "        for content in output_message[\"content\"]:\n",
    "            if \"toolUse\" in content:\n",
    "                tool = content[\"toolUse\"]\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"index\": len(tool_calls),\n",
    "                        \"id\": tool[\"toolUseId\"],\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": tool[\"name\"],\n",
    "                            \"arguments\": json.dumps(tool[\"input\"]),\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # 5. Update Langfuse with response metadata\n",
    "    langfuse_context.update_current_observation(\n",
    "        output=tool_calls,\n",
    "        usage={\n",
    "            \"input\": response[\"usage\"][\"inputTokens\"],\n",
    "            \"output\": response[\"usage\"][\"outputTokens\"],\n",
    "            \"total\": response[\"usage\"][\"totalTokens\"],\n",
    "        },\n",
    "        metadata={\n",
    "            \"ResponseMetadata\": response[\"ResponseMetadata\"],\n",
    "            **metadata,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: LiteLLM Proxy\n",
    "When using / evaluating multiple model providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Any, Generator\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langfuse.client import PromptClient\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "\n",
    "import litellm\n",
    "import litellm.types\n",
    "import litellm.types.utils\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# set callbacks\n",
    "litellm.success_callback = [\"langfuse\"]\n",
    "litellm.failure_callback = [\"langfuse\"]\n",
    "\n",
    "\n",
    "@observe(name=\"LiteLLM Wrapper\")\n",
    "def fn(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model_id: str = \"us.amazon.nova-pro-v1:0\",\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    metadata: Dict[str, Any] = {},\n",
    "    generation_id: Optional[str] = None,\n",
    "    **kwargs,\n",
    ") -> Optional[str]:\n",
    "\n",
    "    metadata[\"existing_trace_id\"] = langfuse_context.get_current_trace_id()\n",
    "    metadata[\"parent_observation_id\"] = langfuse_context.get_current_observation_id()\n",
    "    metadata[\"generation_name\"] = \"LiteLLM Bedrock Converse\"\n",
    "\n",
    "    if generation_id:\n",
    "        metadata[\"generation_id\"] = generation_id  # override langfuse Generation ID\n",
    "    if prompt:\n",
    "        metadata[\"prompt\"] = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=f\"bedrock/converse/{model_id}\",\n",
    "        messages=messages,\n",
    "        metadata=metadata,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@observe(name=\"LiteLLM Streaming Wrapper\")\n",
    "def streaming_fn(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model_id: str = \"us.amazon.nova-pro-v1:0\",\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    metadata: Dict[str, Any] = {},\n",
    "    generation_id: Optional[str] = None,\n",
    "    **kwargs,\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Simple streaming wrapper that only yields text chunks. \n",
    "    See https://docs.litellm.ai/docs/completion/stream#streaming-responses for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata[\"existing_trace_id\"] = langfuse_context.get_current_trace_id()\n",
    "    metadata[\"parent_observation_id\"] = langfuse_context.get_current_observation_id()\n",
    "    metadata[\"generation_name\"] = \"LiteLLM Bedrock Converse\"\n",
    "\n",
    "    if generation_id:\n",
    "        metadata[\"generation_id\"] = generation_id  # override langfuse Generation ID\n",
    "    if prompt:\n",
    "        metadata[\"prompt\"] = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=f\"bedrock/converse/{model_id}\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        metadata=metadata,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    for part in response:\n",
    "        yield part[\"choices\"][0][\"delta\"][\"content\"] or \"\"\n",
    "\n",
    "\n",
    "@observe(name=\"LiteLLM Tool Use Wrapper\")\n",
    "def tool_use_fn(\n",
    "    messages: List[Dict[str, str]],\n",
    "    tools: List[Dict[str, str]],\n",
    "    tool_choice: str = \"auto\",\n",
    "    model_id: str = \"us.amazon.nova-pro-v1:0\",\n",
    "    prompt: Optional[PromptClient] = None,\n",
    "    metadata: Dict[str, Any] = {},\n",
    "    generation_id: Optional[str] = None,\n",
    "    **kwargs,\n",
    ") -> List[litellm.types.utils.ChatCompletionMessageToolCall]:\n",
    "\n",
    "    metadata[\"existing_trace_id\"] = langfuse_context.get_current_trace_id()\n",
    "    metadata[\"parent_observation_id\"] = langfuse_context.get_current_observation_id()\n",
    "    metadata[\"generation_name\"] = \"LiteLLM Bedrock Converse\"\n",
    "\n",
    "    if generation_id:\n",
    "        metadata[\"generation_id\"] = generation_id  # override langfuse Generation ID\n",
    "    if prompt:\n",
    "        metadata[\"prompt\"] = prompt\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=f\"bedrock/converse/{model_id}\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=tool_choice,\n",
    "        metadata=metadata,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return response.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing simple chat\n",
    "@observe(name=\"Simple Chat Example\")\n",
    "def call_chat_api(messages: list[str]):\n",
    "    return fn(messages)\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly AWS GenAI solution architect. You are at the AWS Summit Sydney and you will answer customer's question concisely under 200 words.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is GenAIOps?\"},\n",
    "]\n",
    "\n",
    "# Call the function\n",
    "response = call_chat_api(messages=messages)\n",
    "\n",
    "# Print the response and please check the Langfuse console for the trace\n",
    "print(response)\n",
    "\n",
    "# (Optional) force a flush to update the tracing immediately\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing simple chat\n",
    "@observe(name=\"Simple Chat with Guardrails Example\")\n",
    "def call_chat_api_with_guardrails(messages: list[str]):\n",
    "    return fn(\n",
    "        messages,\n",
    "        guardrailConfig={\n",
    "            \"guardrailIdentifier\": \"<Guardrail ID>\",  # TODO: Create your own guardrail and fill in the ID\n",
    "            \"guardrailVersion\": \"1\",\n",
    "            \"trace\": \"enabled\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly AWS GenAI solution architect. You are at the AWS Summit Sydney and you will answer customer's question concisely under 200 words.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Who are the founding members of AWS?\"},\n",
    "]\n",
    "\n",
    "# Call the function\n",
    "response = call_chat_api_with_guardrails(messages=messages)\n",
    "\n",
    "# Print the response and please check the Langfuse console for the trace\n",
    "print(response)\n",
    "\n",
    "# (Optional) force a flush to update the tracing immediately\n",
    "langfuse_context.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with simple text streaming\n",
    "@observe(name=\"Streaming Chat Example\")\n",
    "def call_streaming_chat_api(messages: List[Dict[str, str]]):\n",
    "    response = \"\"\n",
    "    for chunk in streaming_fn(messages):\n",
    "        print(chunk, end='', flush=True)\n",
    "        response += chunk\n",
    "    return response\n",
    "\n",
    "# Test the streaming function\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly AWS GenAI solution architect. You are at the AWS Summit Sydney and you will answer customer's question concisely under 200 words.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is GenAIOps?\"},\n",
    "]\n",
    "\n",
    "response = call_streaming_chat_api(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"Context Retrieval\")\n",
    "def retrieve_context(city: str) -> str:\n",
    "    \"\"\"Dummy function to retrieve context for the given city.\"\"\"\n",
    "    context = \"\"\"\\\n",
    "21st November 2024\n",
    "Sydney: 24 degrees celcius.\n",
    "New York: 13 degrees celcius.\n",
    "Tokyo: 11 degrees celcius.\"\"\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "@observe(name=\"RAG Example\")\n",
    "def call_rag_api(\n",
    "    query: str,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    ") -> Tuple[str]:\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        tags=[\"dev\"],\n",
    "    )\n",
    "\n",
    "    retrieved_context = retrieve_context(query)\n",
    "    # without langfuse prompt manager\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": f\"Context: {retrieved_context}\\nBased on the context above, answer the following question:\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "        {\"content\": query, \"role\": \"user\"},\n",
    "    ]\n",
    "\n",
    "    # with langfuse prompt manager\n",
    "    # qa_with_context_prompt = langfuse.get_prompt(\"qa-with-context\", version=1)\n",
    "    # messages = qa_with_context_prompt.compile(\n",
    "    #     retrieved_context=retrieved_context,\n",
    "    #     query=query,\n",
    "    # )\n",
    "\n",
    "    trace_id = langfuse_context.get_current_trace_id()\n",
    "    generation_id = uuid.uuid4().hex\n",
    "\n",
    "    return (\n",
    "        fn(\n",
    "            messages,\n",
    "            # prompt=qa_with_context_prompt, # uncomment to link the prompt\n",
    "            # if using LiteLLM functions, pass it down to LiteLLM completion\n",
    "            # generation_id=generation_id,\n",
    "            # if not using LiteLLM, auto-overrides id for functions wrapped with @observe\n",
    "            langfuse_observation_id=generation_id,\n",
    "        ),\n",
    "        trace_id,\n",
    "        generation_id,\n",
    "    )  # return id for async scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    call_rag_api(query=\"What is the temperature in Sydney?\", user_id=\"tenant1-user1\")[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to create a chat prompt\n",
    "langfuse.create_prompt(\n",
    "    name=\"qa-with-context\",\n",
    "    type=\"chat\",\n",
    "    prompt=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Context: {{retrieved_context}}\\nBased on the context above, answer the following question:\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"{{query}}\"},\n",
    "    ],\n",
    "    config={\n",
    "        \"model\": \"amazon.nova-pro-v1:0\",\n",
    "        \"temperature\": 0.1,\n",
    "    },  # optionally, add configs (e.g. model parameters or model tools) or tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_context_prompt = langfuse.get_prompt(\"qa-with-context\", version=1)\n",
    "messages = qa_with_context_prompt.compile(\n",
    "    retrieved_context=\"<context>\",\n",
    "    query=\"<query>\",\n",
    ")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring from backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "output, trace_id, generation_id = call_rag_api(\n",
    "    query=\"What is the temperature in Sydney?\", user_id=\"tenant1-user1\"\n",
    ")\n",
    "\n",
    "# Score the trace from outside the trace context using the low-level SDK\n",
    "# auto evals, score against both observation and trace\n",
    "langfuse.score(\n",
    "    trace_id=trace_id,\n",
    "    observation_id=generation_id,\n",
    "    name=\"accuracy\",\n",
    "    value=random.uniform(0, 1),\n",
    ")\n",
    "\n",
    "# user feedback\n",
    "langfuse.score(\n",
    "    trace_id=trace_id,\n",
    "    name=\"like\",\n",
    "    data_type=\"BOOLEAN\",\n",
    "    value=True,\n",
    "    comment=\"I like how detailed the notes are\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring from frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web SDK example for scoring:\n",
    "* https://langfuse.com/docs/scores/user-feedback#example-using-langfuseweb\n",
    "* https://langfuse.com/docs/sdk/typescript/guide-web\n",
    "\n",
    "```javascript\n",
    "import { LangfuseWeb } from \"langfuse\";\n",
    " \n",
    "export function UserFeedbackComponent(props: { traceId: string }) {\n",
    "  const langfuseWeb = new LangfuseWeb({\n",
    "    publicKey: env.NEXT_PUBLIC_LANGFUSE_PUBLIC_KEY,\n",
    "  });\n",
    " \n",
    "  const handleUserFeedback = async (value: number) =>\n",
    "    await langfuseWeb.score({\n",
    "      traceId: props.traceId,\n",
    "      name: \"user_feedback\",\n",
    "      value,\n",
    "    });\n",
    " \n",
    "  return (\n",
    "    <div>\n",
    "      <button onClick={() => handleUserFeedback(1)}>üëç</button>\n",
    "      <button onClick={() => handleUserFeedback(0)}>üëé</button>\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run the following cell **ONCE** to create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"city_temperature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following code to create a dataset and upload items to it\n",
    "langfuse.create_dataset(name=dataset_name)\n",
    "\n",
    "context = retrieve_context(\"What's the temperature?\")\n",
    "# example items, could also be json instead of strings\n",
    "local_items = [\n",
    "    {\n",
    "        \"input\": {\"context\": context, \"city\": \"Sydney\"},\n",
    "        \"expected_output\": \"24 degrees celcius\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"context\": context, \"city\": \"New York\"},\n",
    "        \"expected_output\": \"13 degrees celcius\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"context\": context, \"city\": \"Tokyo\"},\n",
    "        \"expected_output\": \"11 degrees celcius\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Upload to Langfuse\n",
    "for item in local_items:\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=dataset_name,\n",
    "        # any python object or value\n",
    "        input=item[\"input\"],\n",
    "        # any python object or value, optional\n",
    "        expected_output=item[\"expected_output\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from langfuse.model import DatasetStatus\n",
    "\n",
    "\n",
    "def custom_evaluate(context, query, expected_output, output) -> Tuple[float, str]:\n",
    "    # TODO: define any custom evaluation logic here\n",
    "    # For example, rule-based, LLM-as-judge\n",
    "    return random.uniform(0, 1), \"This is a dummy LLM evaluation\"\n",
    "\n",
    "\n",
    "def run_experiment(run_name: str, user_prompt: str):\n",
    "    \"\"\"\n",
    "    Link score to the dataset item. See https://langfuse.com/docs/datasets/python-cookbook for more details.\n",
    "    \"\"\"\n",
    "    dataset = langfuse.get_dataset(dataset_name)\n",
    "\n",
    "    for item in dataset.items:\n",
    "        if item.status is not DatasetStatus.ACTIVE:\n",
    "            print(f\"Skipping {item.id} of status {item.status}\")\n",
    "            continue\n",
    "        \n",
    "        with item.observe(run_name=run_name) as trace_id:\n",
    "            print(item.input)\n",
    "            context = item.input[\"context\"]\n",
    "            city = item.input[\"city\"]\n",
    "            query = user_prompt.format(city=city)\n",
    "            expected_output = item.expected_output\n",
    "\n",
    "            output, _, _ = call_rag_api(query=query, user_id=\"evals\")\n",
    "\n",
    "            # evaluation logic\n",
    "            score, comment = custom_evaluate(context, query, expected_output, output)\n",
    "\n",
    "            # # surface the score and comment at trace level\n",
    "            langfuse.score(\n",
    "                trace_id=trace_id,\n",
    "                name=\"accuracy\",\n",
    "                data_type=\"NUMERIC\",\n",
    "                value=score,\n",
    "                comment=comment,\n",
    "            )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langfuse.decorators import langfuse_context\n",
    "\n",
    "run_experiment(\n",
    "    run_name=f\"generic_ask_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
    "    user_prompt=\"What is the temperature in {city}?\",\n",
    ")\n",
    "run_experiment(\n",
    "    run_name=f\"precise_ask_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
    "    user_prompt=\"What is the temperature in {city}? Respond with the temperature only.\",\n",
    ")\n",
    "\n",
    "# Assert that all events were sent to the Langfuse API\n",
    "langfuse_context.flush()\n",
    "langfuse.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
