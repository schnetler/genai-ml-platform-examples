{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Retrieval-Augmented Generation (RAG) pipelines with Ragas and Langfuse\n",
    "\n",
    "In this notebook we'll explore ways to evaluate the quality of Retrieval-Augmented Generation (RAG) pipelines with the opensource tools like [RAGAS](https://docs.ragas.io/en/v0.1.21/index.html) and leverage the features in [Langfuse](https://langfuse.com/) to manage and trace the RAG pipelines with traces and spans. We will create a Bedrock knowledge base and the RAG batch generation results to show offline evaluation and scoring.\n",
    "\n",
    "> ‚ÑπÔ∏è Note: This notebook requires user configurations for some steps. \n",
    ">\n",
    "> When a cell requires user configurations, you will see a message like this callout with the üëâ emoji.\n",
    ">\n",
    "> Pay attention to the instructions with the üëâ emoji and perform the configurations in the AWS Console or in the corresponding cell before running the code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "> If you haven't selected the kernel, please click on the \"Select Kernel\" button at the upper right corner, select Python Environments and choose \".venv (Python 3.9.20) .venv/bin/python Recommended\".\n",
    "\n",
    "> To execute each notebook cell, press Shift + Enter.\n",
    "\n",
    "> ‚ÑπÔ∏è You can **skip these prerequisite steps** if you're in an instructor-led workshop using temporary accounts provided by AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional permissions for Amazon OpenSearch\n",
    "\n",
    "To complete the manual Bedrock Knowledge setup steps in this notebook, your **AWS Console user/role** will need:\n",
    "\n",
    "- [Permissions to work with Amazon OpenSearch vector collections](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vector-search.html)\n",
    "- Permission to **create IAM roles** and attach policies to them, including: `iam:AttachRolePolicy`, `iam:CreateRole`, `iam:DetachRolePolicy`, `iam:GetRole`, `iam:PassRole`, `iam:CreatePolicy`, `iam:CreatePolicyVersion`, and `iam:DeletePolicyVersion`.\n",
    "\n",
    "> ‚ÑπÔ∏è **Note:** In testing, we saw `NetworkError` issues when attempting to create Bedrock KBs using only the above-linked `aoss` policy statements. This was resolved by granting `aoss:*` on `*` instead, but you should consider reducing these permissions before using in production environments.\n",
    "\n",
    "Refer to the [AWS Console for Identity and Access Management (IAM)](https://console.aws.amazon.com/iam/home?#/home) to grant permissions to your user or role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install dependencies if you are not using AWS workshop environment\n",
    "# %pip install langfuse datasets ragas python-dotenv langchain-aws boto3 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you have completed the prerequisites to setup the Langfuse project and API keys in the .env file to connect to self-hosted or cloud Langfuse environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already define the environment variables in the .env of the vscode server, please skip the following cell\n",
    "# Define the environment variables for langfuse\n",
    "# You can find those values when you create the API key in Langfuse\n",
    "# import os\n",
    "# os.environ[\"LANGFUSE_SECRET_KEY\"] = \"xxxx\" # Your Langfuse project secret key\n",
    "# os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"xxxx\" # Your Langfuse project public key\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"xxx\" # Langfuse domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Langfuse documentation](https://langfuse.com/docs/get-started) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Authentication Check\n",
    "Run the following cells to initialize common libraries and clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import pandas as pd  # For working with tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize AWS Bedrock clients and check models available in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3  # General Python SDK for AWS (including Bedrock)\n",
    "\n",
    "# used to access Bedrock configuration\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-west-2\")\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\n",
    "    service_name=\"bedrock-agent-runtime\", region_name=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# Check which models are available in your account\n",
    "models = bedrock.list_inference_profiles()\n",
    "for model in models[\"inferenceProfileSummaries\"]:\n",
    "    print(model[\"inferenceProfileName\"] + \" - \" + model[\"inferenceProfileId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Langfuse client and check credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse has been set up correctly\")\n",
    "    print(f\"You can access your Langfuse instance at: {os.environ['LANGFUSE_HOST']}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Credentials not found or invalid. Check your Langfuse API key and host in the .env file.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Knowledge Base\n",
    "Next, let's upload the documents to Amazon S3 and create a vector store (knowledge base) so we can perform retrieval-augmented generation (RAG) given a user query. In the following steps, we'll configure:\n",
    "\n",
    "- An Amazon S3 bucket_name to store our document corpus. \n",
    "- A folder prefix under the bucket where artifacts will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "botosess = boto3.Session(region_name=\"us-west-2\")\n",
    "region = botosess.region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "bucket_name = f\"eval-{account_id}-{region}\"\n",
    "s3_prefix = \"bedrock-rag-eval\"\n",
    "\n",
    "# check if s3 bucket exists or not, if not, create bucket\n",
    "s3 = boto3.client(\"s3\")\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket {bucket_name} exists\")\n",
    "except ClientError:\n",
    "    print(f\"Creating bucket {bucket_name}\")\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket_name, CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the documents to Amazon S3\n",
    "\n",
    "First, we'll need to upload the sample documents to Amazon S3 - for which you can just run the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_s3uri = f\"s3://{bucket_name}/{s3_prefix}/corpus\"\n",
    "print(f\"Syncing corpus to:\\n{corpus_s3uri}/\")\n",
    "\n",
    "# We will use the AWS CLI to recursively sync the folder to the S3 bucket.\n",
    "!aws s3 sync --quiet ./datasets/corpus {corpus_s3uri}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the knowledge base in AWS Console\n",
    "> üëâ This section includes steps you'll need to take manually, not just running the code cells!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to set up the actual Bedrock Knowledge Base for testing is **manually through the AWS Console**:\n",
    "\n",
    "1. First, **open** the [AWS Console for Amazon Bedrock](https://console.aws.amazon.com/bedrock/home?#/knowledge-bases) and select *Orchestration > Knowledge bases* from the left sidebar menu, as shown in the screenshot below:\n",
    "\n",
    "    > ‚ÑπÔ∏è **Check** you're working in the correct *AWS Region* in the top right corner of the UI\n",
    "\n",
    "![KB Console](images/bedrock-kbs/01-bedrock-kb-console.png \"Screenshot of AWS Console for Amazon Bedrock Knowledge Bases, showing 'Create knowledge base' action button\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Click the **Create knowledge base** button and select **Knowledge Base with vector store**. In the screen that opens:\n",
    "\n",
    "- For **knowledge base name**, enter `example-squad-kb`\n",
    "- For **knowledge base description**, you can provide (something like) `Demo knowledge base for question answering evaluation`\n",
    "- Leave the other settings as default (allow creating a new execution role, and no tags)\n",
    "- Please chose Amazon S3 as the data source (default)\n",
    "\n",
    "Your configuration should look like the screenshot below:\n",
    "\n",
    "![KB Basics](images/bedrock-kbs/02a-create-kb-basics.png \"Screenshot of step 1 in Bedrock Knowledge Base creation workflow: with KB name, description, (create new) execution role, and (empty) tags configured. At the end of the form, a 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the **Next** screen, you'll configure the S3 data source.\n",
    "\n",
    "    Leave the data source as S3 and then select the bucket and prefix per you created in the previous step and use Amazon Bedrock default parser.\n",
    "\n",
    "![](images/bedrock-kbs/02b-create-kb-data-source.png \"Screenshot of Knowledge Base vector index settings including Cohere Embed Multilingual embedding model, and quick-create vector store. 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In the **Next** screen, you'll configure the vector index:\n",
    "\n",
    "    For *embeddings model*, select `Cohere Embed Multilingual`\n",
    "\n",
    "    > ‚ÑπÔ∏è **Check** in the [Amazon Bedrock Model Access console](https://console.aws.amazon.com/bedrock/home?#/modelaccess) that you've enabled access to this model in the current region.\n",
    "    >\n",
    "    > If needed, you should be able to select an alternative embedding model instead.\n",
    "\n",
    "    For *Vector database*, select `Quick create a new vector store`\n",
    "\n",
    "    You can find more information from this screen or the [Amazon Bedrock Developer Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-setup.html) about the different vector stores Bedrock Knowledge Bases support. This default option will create a new [Amazon OpenSearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html) cluster\n",
    "\n",
    "    Leave other settings at their defaults as shown below, and you should be ready to proceed:\n",
    "\n",
    "![](images/bedrock-kbs/02c-create-kb-index.png \"Screenshot of Knowledge Base vector index settings including Cohere Embed Multilingual embedding model, and quick-create vector store. 'Next' button is visible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Click **Next** to review your configuration, and then **Create knowledge base** to complete the process.\n",
    "\n",
    "    > ‚è∞ It might take **a few minutes** for the creation to complete. A progress indicator banner should be visible if you scroll up. Alternatively in a separate tab, you could check the [Amazon OpenSearch Serverless Collections console](https://console.aws.amazon.com/aos/home?#opensearch/collections) - where you should see the underlying vector collection being created.\n",
    "\n",
    "    Once your Knowledge Base is completed successfully, you'll be directed to the its detail screen as shown below:\n",
    "\n",
    "![](images/bedrock-kbs/03-kb-detail-page.png \"Detail screen for the created Amazon Bedrock Knowledge Base, showing creation success banner. Includes sections 'Knowledge base overview' (containing the KB ID, name, and other details); 'Tags' (empty); 'Data source' (one Amazon S3 data source listed); 'Embeddings model' (Cohere Embed); and an interactive 'Test knowledge base' chat sidebar on the right with a warning that some data sources have not been synced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. As mentioned in the alert box shown ahead, your new knowledge base will not yet contain your documents until we **sync** the data source:\n",
    "\n",
    "    **Select** your S3 data source by selecting the checkbox to the left of it's name in the data sources list, and click the **Sync** button above to start the sync.\n",
    "\n",
    "    The *Status* will change to `Syncing` for a few seconds, after which it will return to `Available`\n",
    "\n",
    "![](images/bedrock-kbs/04a-kb-data-source-after-sync.png \"Screenshot of KB 'data source' section after running sync, with the data source selected and status showing as 'available'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the sync completed, your Knowledge Base should be ready to use.\n",
    "\n",
    "Optionally, you can click into your data source to check the sync `Added` the 20 files as expected:\n",
    "\n",
    "<img src=\"images/bedrock-kbs/04b-kb-data-sync-details.png\" width=\"600\" alt=\"Data source details screen showing sync completed successfully with 20 files detected and added to the index, and 0 files failed\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the Knowledge Base\n",
    "\n",
    "Before we discuss evaluation at scale, let's run a test queries to check the KB is working properly. Let's go back to the detail page of the knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you can find the knowledge base id is `Z746ERZP5X` in the screenshot below (please check your own *Knowledge Base ID*) on the top of the page in the *Knowledge Base overview* panel.\n",
    "\n",
    "![](images/bedrock-kbs/04c-kb-main-page.png \"Screenshot of the main page of the knowledge base\")\n",
    "\n",
    "üëâ **Replace** the below placeholder with your knowledge base's unique ID, and run the cells below to continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_id = \"<TO FILL>\"  # Something like \"Z746ERZP5X\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ID identified, you can use the Bedrock runtime [RetrieveAndGenerate API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_RetrieveAndGenerate.html) to query your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What kind of economy does Victoria have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the RetrieveAndGenerate API with Nova Pro model to query the knowledge base\n",
    "rag_resp = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\"text\": query},\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            \"knowledgeBaseId\": knowledge_base_id,\n",
    "            \"modelArn\": f\"arn:aws:bedrock:us-west-2:{account_id}:inference-profile/us.amazon.nova-pro-v1:0\",\n",
    "        },\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "    },\n",
    "    # Optional session ID can help improve results for follow-up questions:\n",
    "    # sessionId='string'\n",
    ")\n",
    "\n",
    "print(\"Plain text response:\")\n",
    "print(\"--------------------\")\n",
    "print(rag_resp[\"output\"][\"text\"], end=\"\\n\\n\\n\")\n",
    "\n",
    "print(\"Full API output:\")\n",
    "print(\"----------------\")\n",
    "rag_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the full API response from the above cell, the `RetrieveAndGenerate` action provides:\n",
    "\n",
    "- The final text answer\n",
    "- The `retrievedReferences` from the search engine\n",
    "- Specific `citations` localizing which references should be cited by different parts of the text answer\n",
    "\n",
    "\n",
    "It's also possible to run **only the retrieval** through the API, and skip the generative answer synthesis step - as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_resp = bedrock_agent_runtime.retrieve(\n",
    "    knowledgeBaseId=knowledge_base_id,\n",
    "    retrievalQuery={\"text\": query},\n",
    ")\n",
    "print(json.dumps(retrieve_resp[\"retrievalResults\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset and metrics for evaluation\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "For this example, we are going to use a dataset with reference input/output pairs by querying a RAG system and curating the results. See below for instruction on how to fetch your production data from Langfuse.\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- `question`: list[str] - These are the questions your RAG pipeline will be evaluated on.\n",
    "\n",
    "- `contexts`: list[list[str]] - The contexts which were passed into the LLM to answer the question.\n",
    "\n",
    "- `answer`: list[str] - The answer generated from the RAG pipeline and given to the user.\n",
    "\n",
    "- `ground_truths`: list[list[str]] - The ground truth answer to the questions. However, this can be ignored for online evaluations since we will not have access to ground-truth data in our case.\n",
    "\n",
    "For the details of this dataset, please refer to [Exploding Gradients Dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")[\"baseline\"]\n",
    "fiqa_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS metrics\n",
    "We're going to measure the following aspects of a RAG system. These metrics are defined in [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/):\n",
    "\n",
    "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/): This measures the factual consistency of the generated answer against the given context.\n",
    "- [Response relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/): The ResponseRelevancy metric measures how relevant a response is to the user input.\n",
    "- [Context precision](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/): Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked high. Ideally all the relevant chunks must appear at the top ranks.\n",
    "\n",
    "Checkout the [RAGAS documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/) to know more about these metrics and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    ")\n",
    "\n",
    "# metrics you chose\n",
    "metrics = [\n",
    "    Faithfulness(),\n",
    "    ResponseRelevancy(),\n",
    "    LLMContextPrecisionWithoutReference(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n",
    "\n",
    "\n",
    "# util function to init Ragas Metrics\n",
    "def init_ragas_metrics(metrics, llm, embedding):\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, MetricWithLLM):\n",
    "            print(metric.name + \" llm\")\n",
    "            metric.llm = llm\n",
    "        if isinstance(metric, MetricWithEmbeddings):\n",
    "            print(metric.name + \" embedding\")\n",
    "            metric.embeddings = embedding\n",
    "        run_config = RunConfig()\n",
    "        metric.init(run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to initialize the metrics with LLMs and embedding models of your choice. In this example we are going to use the Bedrock Nova Pro model and Cohere embedding English model, and use the convenience wrappers from the `langchain-aws` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings, ChatBedrockConverse\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "config = {\n",
    "    \"region_name\": \"us-west-2\",  # E.g. \"us-east-1\"\n",
    "    \"llm\": \"us.amazon.nova-pro-v1:0\",  # E.g you can also use the claude models \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "    \"embeddings\": \"cohere.embed-english-v3\",  # E.g or \"amazon.titan-embed-text-v2:0\"\n",
    "    \"temperature\": 0.4,\n",
    "}\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatBedrockConverse(\n",
    "        region_name=config[\"region_name\"],\n",
    "        base_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n",
    "        model=config[\"llm\"],\n",
    "        temperature=config[\"temperature\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(\n",
    "    BedrockEmbeddings(\n",
    "        region_name=config[\"region_name\"],\n",
    "        model_id=config[\"embeddings\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "init_ragas_metrics(\n",
    "    metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embedding=evaluator_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace eval results with Langfuse\n",
    "\n",
    "You can use model-based evaluation with Ragas in 2 ways:\n",
    "1. Score every trace: This means you will run the evaluations for each trace item. This gives you much better idea of how each call made to your RAG pipelines is performing, but please be mindful of the cost.\n",
    "\n",
    "2. Score with sampling: In this method we will take random samples of traces on a periodic basis and score them. This brings down the cost and gives you a rough estimate the performance of your app but may miss out on important samples.\n",
    "\n",
    "In this example, we will demonstrate both solutions using prebuilt dataset and a live RAG pipeline with Bedrock Knowlegebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score every trace\n",
    "\n",
    "Lets take a small example of a single trace and see how you can score that with Ragas. We first define a utility function to score your trace with the metrics you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "\n",
    "async def score_with_ragas(query, chunks, answer, metrics):\n",
    "    scores = {}\n",
    "    for metric in metrics:\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=query,\n",
    "            retrieved_contexts=chunks,\n",
    "            response=answer,\n",
    "        )\n",
    "        print(f\"calculating {metric.name}\")\n",
    "        scores[metric.name] = await metric.single_turn_ascore(sample)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring sample dataset item\n",
    "\n",
    "You compute the score with each request. Below we will go through a dummy application that does the following steps:\n",
    "\n",
    "- Gets a question from the user\n",
    "- Fetch context from the database or vector store that can be used to answer the question from the user\n",
    "- Pass the question and the contexts to the LLM to generate the answer\n",
    "\n",
    "In this case we are demonstrating the use of the Langfuse Python [low-level SDK](https://langfuse.com/docs/sdk/python/low-level-sdk) to log the traces with more granular controls. You can also see an example with the [decorator](https://langfuse.com/docs/sdk/python/decorators) in the later section or read more about them the [langfuse documentation](https://langfuse.com/docs/sdk/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new trace when you get a question\n",
    "row = fiqa_eval[0]\n",
    "question = row[\"question\"]\n",
    "trace = langfuse.trace(name=\"rag-fiqa\")\n",
    "\n",
    "# retrieve the relevant chunks\n",
    "# chunks = get_similar_chunks(question)\n",
    "contexts = row[\"contexts\"]\n",
    "# pass it as span\n",
    "trace.span(\n",
    "    name=\"retrieval\", input={\"question\": question}, output={\"contexts\": contexts}\n",
    ")\n",
    "\n",
    "# use llm to generate a answer with the chunks\n",
    "# answer = get_response_from_llm(question, chunks)\n",
    "answer = row[\"answer\"]\n",
    "trace.generation(\n",
    "    name=\"generation\",\n",
    "    input={\"question\": question, \"contexts\": contexts},\n",
    "    output={\"answer\": answer},\n",
    ")\n",
    "\n",
    "# compute scores for the question, context, answer tuple\n",
    "ragas_scores = await score_with_ragas(question, contexts, answer, metrics)\n",
    "ragas_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Now you can see this is traced in langfuse but with no score attached, we can check it in the Langfuse UI at:\\n{os.environ['LANGFUSE_HOST']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then attach the scores to the trace by running the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the scores\n",
    "for m in metrics:\n",
    "    trace.score(name=m.name, value=ragas_scores[m.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the score is attached\n",
    "\n",
    "![](images/bedrock-kbs/04e-langfuse-single-eval-trace-score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring RAG\n",
    "We have already setup the Bedrock Knowledge Base in the first section, we can now **evaluate** the quality of its results against a test dataset - to help us **optimize** the configuration for high quality and low cost.\n",
    "\n",
    "First, let's load the sample dataset of questions, reference answers, and their source documents (to find more of how to prepare this dataset, please see more details in [this github](https://github.com/aws-samples/llm-evaluation-methodology/blob/main/datasets/Prepare-SQuAD.ipynb)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_json(\"datasets/qa.manifest.jsonl\", lines=True)\n",
    "dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records in this dataset include:\n",
    "\n",
    "- (`doc`) The full text of the source document for this example\n",
    "- (`doc_id`) A unique identifier for the source document\n",
    "- (`question`) The user question to be asked\n",
    "- (`question_id`) A unique identifier for the question\n",
    "- (`answers`) A list of (possibly multiple) reference 'correct' answers, supported by the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in [Ragas' API Reference](https://docs.ragas.io/en/latest/references/evaluation.html), records in Ragas evaluation datasets typically include:\n",
    "\n",
    "- The `question` that was asked\n",
    "- The `answer` the system generated\n",
    "- The actual text `contexts` the answer was based on (i.e. snippets of document text retrieved by the search engine)\n",
    "- The `ground_truth` answer(s)\n",
    "\n",
    "Here we will integrate [Langfuse Tracking](https://langfuse.com/docs/tracing) into the RAG pipeline with the Langfuse Python SDK using the `@observe()` decorator.\n",
    "\n",
    "We can run an example question through the Bedrock KB retrieve and generate pipeline as shown below, and extract the references ready to calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe, langfuse_context\n",
    "\n",
    "\n",
    "@observe(name=\"Knowledge Base Retrieve and Generate\")\n",
    "def retrieve_and_generate(\n",
    "    question: str,\n",
    "    kb_id: str,\n",
    "    generate_model_arn: str = f\"arn:aws:bedrock:us-west-2:{account_id}:inference-profile/us.amazon.nova-pro-v1:0\",\n",
    "    **kwargs,\n",
    "):\n",
    "    rag_resp = bedrock_agent_runtime.retrieve_and_generate(\n",
    "        input={\"text\": question},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": generate_model_arn,\n",
    "            },\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "        },\n",
    "    )\n",
    "    answer = rag_resp[\"output\"][\"text\"]\n",
    "\n",
    "    # Fetch flat list of references from the nested citations -> retrievedReferences:\n",
    "    all_refs = [\n",
    "        r for cite in rag_resp[\"citations\"] for r in cite[\"retrievedReferences\"]\n",
    "    ]\n",
    "    contexts = [r[\"content\"][\"text\"] for r in all_refs]\n",
    "    ref_s3uris = [r[\"location\"][\"s3Location\"][\"uri\"] for r in all_refs]\n",
    "    # Map e.g. 's3://.../doc_id.txt' to 'doc_id':\n",
    "    ref_ids = [uri.rpartition(\"/\")[2].rpartition(\".\")[0] for uri in ref_s3uris]\n",
    "\n",
    "    # Log additional data to the trace\n",
    "    langfuse_context.update_current_observation(\n",
    "        input={\"question\": question, \"contexts\": contexts},\n",
    "        output=answer,\n",
    "        model=\"us.amazon.nova-pro-v1:0\",\n",
    "        session_id=\"kb-rag-session\",\n",
    "        tags=[\"dev\"],\n",
    "        metadata=kwargs,\n",
    "    )\n",
    "\n",
    "    # Get the trace ID for independent scoring\n",
    "    trace_id = langfuse_context.get_current_trace_id()\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_doc_ids\": ref_ids,\n",
    "        \"retrieved_doc_texts\": contexts,\n",
    "        \"trace_id\": trace_id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RAG as requests come in and score the results immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe, langfuse_context\n",
    "from asyncio import run\n",
    "\n",
    "\n",
    "@observe(name=\"Knowledge Base Pipeline\")\n",
    "def rag_pipeline(\n",
    "    question,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    "    kb_id: Optional[str] = None,\n",
    "    metrics: Optional[Any] = None,\n",
    "):\n",
    "\n",
    "    generated_answer = retrieve_and_generate(\n",
    "        question=question,\n",
    "        kb_id=kb_id,\n",
    "        kwargs={\"database\": \"Bedrock Knowledge Base\", \"kb_id\": kb_id},\n",
    "    )\n",
    "    contexts = generated_answer[\"retrieved_doc_texts\"]\n",
    "    answer = generated_answer[\"answer\"]\n",
    "    trace_id = generated_answer[\"trace_id\"]\n",
    "\n",
    "    score = run(score_with_ragas(question, contexts, answer=answer, metrics=metrics))\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        tags=[\"dev\"],\n",
    "    )\n",
    "    for s in score:\n",
    "        langfuse.score(name=s, value=score[s], trace_id=trace_id)\n",
    "    return generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_pipeline(dataset_df.iloc[0][\"question\"], kb_id=knowledge_base_id, metrics=metrics)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring with sampling\n",
    "\n",
    "Scoring every production trace can be time-consuming and costly depending on your application architecture and traffic. In that case, it's better to start off with a sampling method. Decide a timespan you want to run the batch process and the number of traces you want to sample from that time slice. Create a dataset and call ragas.evaluate to analyze the result.\n",
    "\n",
    "You can run this periodically to keep track of how the scores are changing across timeslices and figure out if there are any discrepancies.\n",
    "\n",
    "We will evaluate the existing results generated previously by the `retrieve_and_generate()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate 10 production traces by running RAG on the first 10 questions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_generated_outputs = [\n",
    "    retrieve_and_generate(\n",
    "        question=rec.question,\n",
    "        kb_id=knowledge_base_id,\n",
    "        kwargs={\"database\": \"Bedrock Knowledge Base\", \"kb_id\": knowledge_base_id},\n",
    "    )\n",
    "    for _, rec in dataset_df.head(10).iterrows()\n",
    "]\n",
    "rag_generated_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the results are uploaded to langfuse you can retrieve it as needed with this handy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.api.resources.commons.types.trace_with_details import TraceWithDetails\n",
    "\n",
    "def get_traces(\n",
    "    limit: int = 5,\n",
    "    name: Optional[str] = None,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    "    from_timestamp: Optional[str] = None,\n",
    "    to_timestamp: Optional[str] = None,\n",
    ") -> List[TraceWithDetails]:\n",
    "    \"\"\"Query Langfuse for traces matching the given filters.\n",
    "    See https://langfuse.com/docs/query-traces for more details.\"\"\"\n",
    "\n",
    "    all_data = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = langfuse.fetch_traces(\n",
    "            page=page,\n",
    "            name=name,\n",
    "            user_id=user_id,\n",
    "            session_id=session_id,\n",
    "            from_timestamp=from_timestamp,\n",
    "            to_timestamp=to_timestamp,\n",
    "        )\n",
    "        if not response.data:\n",
    "            break\n",
    "        page += 1\n",
    "        all_data.extend(response.data)\n",
    "        if len(all_data) > limit:\n",
    "            break\n",
    "\n",
    "    return all_data[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "NUM_TRACES_TO_SAMPLE = 3\n",
    "traces = get_traces(name=\"Knowledge Base Retrieve and Generate\", limit=10)\n",
    "if len(traces) > NUM_TRACES_TO_SAMPLE:\n",
    "    traces_sample = sample(traces, NUM_TRACES_TO_SAMPLE)\n",
    "else:\n",
    "    traces_sample = traces\n",
    "\n",
    "print(f\"Sampled {len(traces_sample)} traces from {len(traces)} filtered traces\")\n",
    "for trace in traces_sample:\n",
    "    print(f\"Trace ID: {trace.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make a batch and score it. Ragas uses huggingface dataset object to build the dataset and run the evaluation. If you run this on your own production data, use the right keys to extract the question, contexts and answer from the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on a sample\n",
    "evaluation_batch = {\n",
    "    \"question\": [],\n",
    "    \"contexts\": [],\n",
    "    \"answer\": [],\n",
    "    \"trace_id\": [],\n",
    "}\n",
    "\n",
    "for sample in traces_sample:\n",
    "    evaluation_batch[\"question\"].append(sample.input[\"question\"])\n",
    "    evaluation_batch[\"contexts\"].append(sample.input[\"contexts\"])\n",
    "    evaluation_batch[\"answer\"].append(sample.output)\n",
    "    evaluation_batch[\"trace_id\"].append(sample.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ragas evaluate function to score an entire dataset instead of single turn. See [Ragas evaluate](https://docs.ragas.io/en/latest/references/evaluate/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ragas evaluate\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy\n",
    "\n",
    "ds = Dataset.from_dict(evaluation_batch)\n",
    "evals_results = evaluate(\n",
    "    ds,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    "    metrics=[Faithfulness(), ResponseRelevancy()],\n",
    ")\n",
    "evals_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it! You can see the scores over a time period. Let's render the results in a dataframe to see the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evals_results.to_pandas()\n",
    "\n",
    "# add the langfuse trace_id to the result dataframe\n",
    "df[\"trace_id\"] = ds[\"trace_id\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push the scores back into Langfuse and attach them to the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    for metric_name in [\"faithfulness\", \"answer_relevancy\"]:\n",
    "        langfuse.score(\n",
    "            name=metric_name, value=row[metric_name], trace_id=row[\"trace_id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now go back to the Langfuse console and check the updated scores in the traces.\n",
    "\n",
    "![](images/bedrock-kbs/score-with-sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratuations!\n",
    "You have successfully finished Lab 2.\n",
    "\n",
    "If you are at an AWS event, you can return to the workshop studio for additional instructions before moving into the next lab, where we will explore model-based evaluation and guardrails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
